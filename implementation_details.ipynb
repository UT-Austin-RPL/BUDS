{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "Note: To render latex formula, open the file with jupyter notebook\n",
    "\n",
    "### Implementation for Multi-sensory Representation Learning\n",
    "\n",
    "The representation learning is formulated as a probabilistic graphical model, the same as in Lee et al. [1]. We aim to learn $p(z|D)$, the posterior distribution of latent variable z using the dataset $D=\\{(o_{i}, y_{i}|i=1, \\dots, N)\\}$. Here $o_{i}$ is the sensor readings of all modalities (images + proprioception), and $y_{i}$ is the self-supervised label, which is the same as $o_{i}$. So as we described in our paper, we optimize over reconstructing the original sensor inputs to capture the statistical patterns that exist among different sensor modalities.  \n",
    "\n",
    "**Loss function** In the fusion module, we have encoders that are parameterized by $\\theta_{e}$, decoders that are parameterized by $\\theta_{d}$. And the loss function for training encoders is:\n",
    "\n",
    "$\\mathcal{L}_{i}(\\theta_{e}, \\theta_{d})=\\mathbb{E}_{q_{\\theta_{e}}}[log_{p_{\\theta_{d}}}(D_{i}|z)]-KL(q_{\\theta_{e}}(z|D_{i})||p(z))$.\n",
    "\n",
    "**Model architecture** We use the same convolutional network structures as the models from\n",
    "Lee et al.[1], except that we do not have skip connections from the\n",
    "encoder to the decoder parts. We choose 32 for the latent dimension\n",
    "of the fused representation. For training the fusion model for each\n",
    "task, we use 1000 training epochs, with a learning rate of 0.001,\n",
    "batch size of 128.\n",
    "\n",
    "\n",
    "### Hyperparameters for Skill Segmentatation.\n",
    "\n",
    "We present the hyperparameters for the unsupervised clustering\n",
    "step. The maximum number of clusters for each task is: 6 for\n",
    "<tt>Tool-Use</tt> and <tt>Hammer-Place</tt>, 8 for <tt>Kitchen</tt>\n",
    "and <tt>Real-Kitchen</tt>, 10 for <tt>Multitask-Kitchen</tt>. And\n",
    "the stopping criteria of the bread-first search is the number of\n",
    "segments of mid-level segments are more than twice the maximum number\n",
    "of clusters. We also use a minimum length threshold to reject a small\n",
    "cluster, the number we choose is: 30 for <tt>Tool-Use</tt>,\n",
    "<tt>Real-Kitchen</tt>, 35 for <tt>Hammer-Place</tt>, 20 for\n",
    "<tt>Multitask-Kitchen</tt>. In this work, these values of\n",
    "hyperparameters are tuned heuristically, and how to extend to an\n",
    "end-to-end method is another future direction to look at.\n",
    "\n",
    "\n",
    "### Hyperparameters for Sensorimotor Policies Models\n",
    "\n",
    "We choose the dimension for subgoal vector $\\omega_t$ to be 32, the\n",
    "number of 2D keypoints from the output of Spatial Softmax layer to\n",
    "be 64. We choose H=30 for all single-task environments (Both\n",
    "simulation and real robots). We choose H=20 for the multitask\n",
    "environment <tt>Multitask-Kitchen</tt>. This is because skills are\n",
    "relatively short in each task in <tt>Multitask-Kitchen</tt> domain\n",
    "compared to all single-task environments.\n",
    "\n",
    "### Training Details for Sensorimotor Policies\n",
    "To increase the generalization ability of the model, we apply data\n",
    "augmentation [2] to images for both training skills and meta\n",
    "controllers. To further increase the robustness of policies\n",
    "$\\pi^{(k)}_{L}$, we also add some noise from Gaussian distribution\n",
    "with a standard deviation of 0.1.\n",
    " \n",
    "For all skills, we train for 2001 epochs with a learning rate of\n",
    "$0.0001$, and the loss function we use is $\\ell_{2}$ loss. We use two\n",
    "layers (300, 400 hidden units for each layer) for the fully\n",
    "connected layers in all sing-task environments, while three layers\n",
    "(300, 300, 400) hidden units for each layer for fully connected\n",
    "layers in <tt>Multitask-Kitchen</tt> domain. For meta controllers, we\n",
    "train 1001 epochs in all simulated single-task environments, 2001\n",
    "epochs in <tt>Multitask-Kitchen</tt> domain, and 3001 epochs in\n",
    "<tt>Real-Kitchen</tt>. For kl coefficients during cVAE training, we\n",
    "choose 0.005 for <tt>Tool-Use</tt>, <tt>Hammer-Place</tt>, and\n",
    "0.01 for all other environments.\n",
    " \n",
    "\n",
    "\n",
    "## References\n",
    "[1] Making sense of vision and touch: Learning multimodal\n",
    "representations for contact-rich tasks. Lee M. et al.\n",
    "\n",
    "\n",
    "[2] Image augmentation is all you need: Regularizing deep\n",
    "reinforcement learning from pixels. Kostrikov I. et al.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
