{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "### Implementation for Multi-sensory Representation Learning\n",
    "We use the same convolutional network structures as the models from\n",
    "Lee et al.[1], except that we do not have skip connections from the\n",
    "encoder to the decoder parts. We choose 32 for the latent dimension\n",
    "of the fused representation. For training the fusion model for each\n",
    "task, we use 1000 training epochs, with a learning rate of 0.001,\n",
    "batch size of 128.\n",
    "\n",
    "\n",
    "### Hyperparameters for Skill Segmentatation.\n",
    "\n",
    "We present the hyperparameters for the unsupervised clustering\n",
    "step. The maximum number of clusters for each task is: 6 for\n",
    "<tt>Tool-Use</tt> and <tt>Hammer-Place</tt>, 8 for <tt>Kitchen</tt>\n",
    "and <tt>Real-Kitchen</tt>, 10 for <tt>Multitask-Kitchen</tt>. And\n",
    "the stopping criteria of the bread-first search is the number of\n",
    "segments of mid-level segments are more than twice the maximum number\n",
    "of clusters. We also use a minimum length threshold to reject a small\n",
    "cluster, the number we choose is: 30 for <tt>Tool-Use</tt>,\n",
    "<tt>Real-Kitchen</tt>, 35 for <tt>Hammer-Place</tt>, 20 for\n",
    "<tt>Multitask-Kitchen</tt>. In this work, these values of\n",
    "hyperparameters are tuned heuristically, and how to extend to an\n",
    "end-to-end method is another future direction to look at.\n",
    "\n",
    "\n",
    "### Model Details for Sensorimotor Policies\n",
    "\n",
    "BUDS focuses on learning closed-loop sensorimotor skills. The input to\n",
    "each skill is the observations from robot sensors and the latent\n",
    "subgoal vector $\\omega_{t}$. Specifically, the observations consist of\n",
    "two RGB images (128 x 128) from the workspace camera and the\n",
    "eye-in-hand camera, and the proprioception of joint and gripper\n",
    "states. For encoding visual inputs, we use ResNet-18 [2] as the visual\n",
    "encoder, followed by Spatial Softmax [3] to extract keypoints of the\n",
    "feature maps. We then concatenate keypoints with proprioception (joint\n",
    "angles and past five frames of gripper states [4]), and concatenated\n",
    "vectors are passed through fully connected layers with LeakyReLU\n",
    "activation, outputting end-effector motor commands. The subgoal\n",
    "encoder $E_k$ is a ResNet-18 module with spatial softmax module, and\n",
    "$E_k$ only takes the image from the workspace camera of the subgoal\n",
    "state in demonstration data as inputs. The meta controller $\\pi_{H}$\n",
    "takes the image of current observation from the workspace camera as\n",
    "input, and the visual encoder in $\\pi_{H}$ is also a ResNet-18\n",
    "module. For all ResNet-18 modules, we remove the last two layers\n",
    "compared to the original design, giving us 4x4 feature\n",
    "maps. For low-level robot controllers, we use position-based\n",
    "Operational Space Controller (OSC) [5] with a binary controller for\n",
    "the parallel-jaw gripper, and the controllers take commands at 20\n",
    "Hz. During evaluation, we choose the meta controller to operate at 4\n",
    "Hz while sensorimotor skills operate at 20 Hz.\n",
    "\n",
    "We choose the dimension for subgoal vector $\\omega_t$ to be 32, the\n",
    "number of 2D keypoints from the output of Spatial Softmax layer to\n",
    "be 64. We choose H=30 for all single-task environments (Both\n",
    "simulation and real robots). We choose H=20 for the multitask\n",
    "environment <tt>Multitask-Kitchen</tt>. This is because skills are\n",
    "relatively short in each task in <tt>Multitask-Kitchen</tt> domain\n",
    "compared to all single-task environments.\n",
    "\n",
    "### Training Details for Sensorimotor Policies\n",
    "To increase the generalization ability of the model, we apply data\n",
    "augmentation [6] to images for both training skills and meta\n",
    "controllers. To further increase the robustness of policies\n",
    "$\\pi^{(k)}_{L}$, we also add some noise from Gaussian distribution\n",
    "with a standard deviation of 0.1.\n",
    " \n",
    "For all skills, we train for 2001 epochs with a learning rate of\n",
    "$0.0001$, and the loss function we use is $\\ell_{2}$ loss. We use two\n",
    "layers (300, 400 hidden units for each layer) for the fully\n",
    "connected layers in all sing-task environments, while three layers\n",
    "(`300`, 300, 400) hidden units for each layer for fully connected\n",
    "layers in <tt>Multitask-Kitchen</tt> domain. For meta controllers, we\n",
    "train 1001 epochs in all simulated single-task environments, 2001\n",
    "epochs in <tt>Multitask-Kitchen</tt> domain, and 3001 epochs in\n",
    "<tt>Real-Kitchen</tt>. For kl coefficients during cVAE training, we\n",
    "choose 0.005 for <tt>Tool-Use</tt>, <tt>Hammer-Place</tt>, and\n",
    "0.01 for all other environments.\n",
    " \n",
    "\n",
    "\n",
    "## References\n",
    "[1] Making sense of vision and touch: Learning multimodal\n",
    "representations for contact-rich tasks. Lee M. et al.\n",
    "\n",
    "[2] Deep residual learning for image recognition. He K. et al.\n",
    "\n",
    "[3] Deep spatial autoencoders for visuomotor learning. Finn C. et al.\n",
    "\n",
    "[4] Deep imitation learning for complex manipulation tasks from\n",
    "virtual reality teleoperation. Zhang T. et al.\n",
    "\n",
    "[5] A unified approach for motion and force control of robot\n",
    "manipulators: The operational space formulation. Khatib O.\n",
    "\n",
    "[6] Image augmentation is all you need: Regularizing deep\n",
    "reinforcement learning from pixels. Kostrikov I. et al.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
